{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = th.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = th.matmul(attention, v)\n",
    "    return values\n",
    "\n",
    "def batched_scaled_dot_product(q,k,v,mask,num_batches=2):\n",
    "    chunked_queries = q.chunk(num_batches, dim=-2)\n",
    "    result = None\n",
    "    for cq in chunked_queries:\n",
    "        current_result = scaled_dot_product(cq, k, v)\n",
    "        if result is None:\n",
    "            result = current_result\n",
    "        else:\n",
    "            result = th.cat((result, current_result), dim=-2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''batch_size = 10\n",
    "lenght = 10000\n",
    "dim=2048\n",
    "\n",
    "q = th.zeros([batch_size,1,dim], device='cuda')\n",
    "k = th.zeros([batch_size,lenght,dim], device='cuda')\n",
    "v = th.zeros([batch_size,lenght,dim], device='cuda')\n",
    "\n",
    "values2 = scaled_dot_product(q,k,v)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import non_hierarchical\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_chunks, batched, last_n=None):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_chunks = num_chunks\n",
    "\n",
    "        self.batched = batched\n",
    "        self.last_n = last_n\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = self.qkv_proj(x)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        if self.last_n is not None:\n",
    "            q = q[:,:,-self.last_n:]\n",
    "            seq_length = self.last_n\n",
    "        # Determine value outputs\n",
    "        if self.batched:\n",
    "            values = batched_scaled_dot_product(q, k, v, mask=mask, num_batches=self.num_chunks)\n",
    "        else:\n",
    "            values = scaled_dot_product(q, k, v)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import result\n",
    "from xml.etree.ElementTree import tostring\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0, batched=True, last_n=None, num_chunks = 10):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads, num_chunks=num_chunks, batched=batched, last_n=last_n)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batched = batched\n",
    "        self.last_n = last_n\n",
    "        self.num_chunks = num_chunks\n",
    "        self.positional_encoding = PositionalEncoding(d_model=input_dim)\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.norm1(x)\n",
    "        x =  x * math.sqrt(self.dim_feedforward)\n",
    "        x = self.positional_encoding.forward(x)\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x)\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(attn_out)\n",
    "        x = self.norm2(x)\n",
    "        result = th.cat((x, linear_out), dim=1)\n",
    "\n",
    "        return result\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 50000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = th.arange(max_len).unsqueeze(1)\n",
    "        div_term = th.exp(th.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = th.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = th.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = th.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mha = MultiheadAttention(1, embed_dim=10, num_heads=2, num_chunks = 10).to('cuda')\n",
    "eb = EncoderBlock(input_dim=2, num_heads=1, dim_feedforward=10, batched=True, last_n=None, num_chunks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongHorizonLearner:\n",
    "    def __init__(self, encoder_block:EncoderBlock) -> None:\n",
    "        self.encoder_block = encoder_block\n",
    "\n",
    "    def forward(self, inpt, obsv):\n",
    "        x = th.cat((inpt, obsv), dim = 1)\n",
    "        output = self.encoder_block.forward(x)\n",
    "        return output\n",
    "\n",
    "    def learn_seqence(self, sequence:th.Tensor, label:th.Tensor):\n",
    "        horizon = th.zeros(sequence.size(0), 1, self.encoder_block.input_dim, device='cuda')\n",
    "        optimizer = th.optim.Adam(self.encoder_block.parameters(), lr=1e-3)\n",
    "        for j in range(10000):\n",
    "            for i in range(len(sequence[0])):\n",
    "                num_chunks = max(horizon.size(1)//10000, 1)\n",
    "                #start = th.cuda.Event(enable_timing=True)\n",
    "                #end = th.cuda.Event(enable_timing=True)\n",
    "                self.encoder_block.self_attn.num_chunks = num_chunks            \n",
    "                #start.record()\n",
    "                horizon = self.forward(horizon.detach(), sequence[:,i].unsqueeze(1))\n",
    "                #end.record()\n",
    "                horizon = th.cat((horizon, label[:,i].unsqueeze(1)), dim=1)\n",
    "                loss = ((horizon[:,-2] - horizon[:,-1])**2)\n",
    "                loss = loss.mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (j*len(sequence[0]) + i) % 1000 ==0:\n",
    "                    print(loss)\n",
    "                    print(num_chunks)\n",
    "                    print(horizon.shape)\n",
    "                    print('_____________________')\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "batch_size = 2\n",
    "inpt_dim = 100\n",
    "eb = EncoderBlock(input_dim=inpt_dim, num_heads=2, dim_feedforward=64, batched=False, last_n=1, num_chunks=1).to('cuda')\n",
    "LHL = LongHorizonLearner(eb)\n",
    "sequence = th.randint(0, 2, [batch_size,seq_len,inpt_dim], device='cuda', dtype=th.bool)\n",
    "label = ~sequence\n",
    "sequence = sequence.type(th.float)\n",
    "label = label.type(th.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ActiveCritic.tests.test_utils.utils import make_seq_encoding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_seq, outpt_seq = make_seq_encoding_data(batch_size=2, seq_len=5, ntoken=inpt_dim, d_out=inpt_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outpt_seq[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 4, 100])\n",
      "_____________________\n",
      "tensor(0.1533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 3004, 100])\n",
      "_____________________\n",
      "tensor(0.1614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 6004, 100])\n",
      "_____________________\n",
      "tensor(0.1188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 9004, 100])\n",
      "_____________________\n",
      "tensor(0.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 12004, 100])\n",
      "_____________________\n",
      "tensor(0.1449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 15004, 100])\n",
      "_____________________\n",
      "tensor(0.1772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1\n",
      "torch.Size([2, 18004, 100])\n",
      "_____________________\n",
      "tensor(0.1513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "torch.Size([2, 21004, 100])\n",
      "_____________________\n",
      "tensor(0.1684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "torch.Size([2, 24004, 100])\n",
      "_____________________\n",
      "tensor(0.1518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "torch.Size([2, 27004, 100])\n",
      "_____________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LHL\u001b[39m.\u001b[39;49mlearn_seqence(sequence\u001b[39m=\u001b[39;49minpt_seq, label\u001b[39m=\u001b[39;49moutpt_seq)\n",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb Cell 11\u001b[0m in \u001b[0;36mLongHorizonLearner.learn_seqence\u001b[0;34m(self, sequence, label)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m (j\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(sequence[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m i) \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MasterProject/long_horizon_transformer.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfTest/lib/python3.9/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfTest/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfTest/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/tfTest/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfTest/lib/python3.9/site-packages/torch/optim/adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    264\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 265\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[1;32m    268\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LHL.learn_seqence(sequence=inpt_seq, label=outpt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = th.ones([2,2,2])\n",
    "result = eb.forward(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "inpt = th.zeros([10,seq_len,1], device='cuda')\n",
    "inpt[:5,0,0] = 1\n",
    "\n",
    "result = th.zeros([10,seq_len,10], device='cuda')\n",
    "result[:5,-1,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =mha.forward(inpt, batched=True, last_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiter = th.optim.Adam(mha.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "start_event = th.cuda.Event(enable_timing=True)\n",
    "end_event = th.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "\n",
    "# Run some things here\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    erg = mha.forward(inpt, batched=True)\n",
    "\n",
    "    loss = ((erg[:,-1,0].reshape(-1) - result[:,-1,0].reshape(-1))**2).mean()\n",
    "    optimiter.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiter.step()\n",
    "\n",
    "    if i%10==0:\n",
    "        print(loss)\n",
    "end_event.record()\n",
    "th.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(elapsed_time_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "start_event = th.cuda.Event(enable_timing=True)\n",
    "end_event = th.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "\n",
    "# Run some things here\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    erg = mha.forward(inpt, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "    loss = ((erg[:,-1,0].reshape(-1) - result[:,-1,0].reshape(-1))**2).mean()\n",
    "    optimiter.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiter.step()\n",
    "\n",
    "    if i%10==0:\n",
    "        print(loss)\n",
    "end_event.record()\n",
    "th.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(elapsed_time_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "optimiter = th.optim.Adam(mha.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    h = time.perf_counter()\n",
    "    erg = mha.forward(inpt, batched=False)\n",
    "    print(time.perf_counter()-h)\n",
    "    print('_______________________')\n",
    "    loss = ((erg[:,-1,0].reshape(-1) - result[:,-1,0].reshape(-1))**2).mean()\n",
    "    optimiter.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiter.step()\n",
    "    if i%10==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erg[0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bee90e249730b85f00f3915f0cf4f21bc0729131dcc7008c941068256fd0d344"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tfTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
